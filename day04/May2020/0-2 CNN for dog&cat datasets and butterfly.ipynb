{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review CNN with Cat and Dog as well as Butterfly by Keras\n",
    "\n",
    "- Sequential model\n",
    "- Functional model\n",
    "\n",
    "## Steps for construction CNN models by Keras\n",
    "\n",
    "#### 1 Data preprocessing\n",
    "\n",
    "- need to match the size of the data with the input size of the CNN model, eg., Conv2D takes ######### and Conv1D takes ########\n",
    "\n",
    "\n",
    "#### 2 Build model\n",
    "\n",
    "#### 3 Compile model\n",
    "\n",
    "#### 4 Fit model\n",
    "\n",
    "#### 5 Prediction\n",
    "\n",
    "#### 6 Evaluation\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN on the data set Dogs&cats \n",
    "\n",
    "### Import libraies / functions / modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
    "IMG_DIM = (150, 150)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1 Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "(3000, 150, 150, 3)\n",
      "Train dataset shape: (3000, 150, 150, 3) \tValidation dataset shape: (1000, 150, 150, 3) \ttest dataset shape: (760, 150, 150, 3)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "_data_path = './asset4/dog_cat/'\n",
    "train_dir = _data_path+\"training_data/\"\n",
    "valid_dir = _data_path+\"validation_data/\"\n",
    "test_dir = _data_path+\"test_data/\"\n",
    "\n",
    "train_files = glob.glob(train_dir+\"*\") \n",
    "print(len(train_files))\n",
    "train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]\n",
    "train_imgs = np.array(train_imgs)\n",
    "train_labels = [fn.split('\\\\')[-1].split('.')[0].strip() for fn in train_files]\n",
    "print(train_imgs.shape)\n",
    "\n",
    "validation_files = glob.glob(valid_dir+'*')\n",
    "validation_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in validation_files]\n",
    "validation_imgs = np.array(validation_imgs)\n",
    "validation_labels = [fn.split('\\\\')[-1].split('.')[0].strip() for fn in validation_files]\n",
    "\n",
    "test_files = glob.glob(test_dir+'*')\n",
    "test_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in test_files]\n",
    "test_imgs = np.array(test_imgs)\n",
    "test_labels = [fn.split('\\\\')[-1].split('.')[0].strip() for fn in test_files]\n",
    "print('Train dataset shape:', train_imgs.shape, \n",
    "      '\\tValidation dataset shape:', validation_imgs.shape,\n",
    "     '\\ttest dataset shape:', test_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "train_labels_enc = le.transform(train_labels)\n",
    "validation_labels_enc = le.transform(validation_labels)\n",
    "test_labels_enc = l e.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_enc\n",
    "# train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen  = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow(train_imgs, train_labels_enc, batch_size=30)\n",
    "val_generator = val_datagen.flow(validation_imgs, validation_labels_enc, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(val_generator)\n",
    "# train_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3 build a Sequential model  and compile the Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Tommy/opt/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 148, 148, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 74, 74, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 72, 72, 64)        9280      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,706,113\n",
      "Trainable params: 3,706,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout,Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "input_shape = (150, 150, 3)\n",
    "# designing the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', \n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# To comile the model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['accuracy'])\n",
    "# To show the construction of model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Fit the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Tommy/opt/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/2\n",
      "50/50 [==============================] - 9s 181ms/step - loss: 5.3441e-07 - acc: 1.0000\n",
      "100/100 [==============================] - 91s 915ms/step - loss: 0.0182 - acc: 0.9900 - val_loss: 5.3441e-07 - val_acc: 1.0000\n",
      "Epoch 2/2\n",
      "50/50 [==============================] - 9s 178ms/step - loss: 1.0786e-07 - acc: 1.0000\n",
      "100/100 [==============================] - 90s 904ms/step - loss: 4.8696e-07 - acc: 1.0000 - val_loss: 1.0786e-07 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator, steps_per_epoch=100, epochs=2,\n",
    "                              validation_data=val_generator, validation_steps=50, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Evaluate on the test data\n",
      "760/760 [==============================] - 2s 2ms/sample - loss: 31.0514 - accuracy: 0.5421\n",
      "loss:31.0514 accuracy:0.5421\n"
     ]
    }
   ],
   "source": [
    "model.predict(test_imgs)\n",
    "print('\\n#Evaluate on the test data')\n",
    "loss, accuracy = model.evaluate(test_imgs,test_labels_enc)\n",
    "print('loss:%.4f accuracy:%.4f'%(loss,accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Visualize the results of training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "t = f.suptitle('Basic CNN Performance', fontsize=12)\n",
    "f.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "epoch_list = list(range(1,3))\n",
    "ax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\n",
    "ax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\n",
    "ax1.set_xticks(np.arange(0, 20, 1))\n",
    "ax1.set_ylabel('Accuracy Value')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_title('Accuracy')\n",
    "l1 = ax1.legend(loc=\"best\")\n",
    "\n",
    "ax2.plot(epoch_list, history.history['loss'], label='Train Loss')\n",
    "ax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\n",
    "ax2.set_xticks(np.arange(0, 20, 1))\n",
    "ax2.set_ylabel('Loss Value')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_title('Loss')\n",
    "l2 = ax2.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex1.  build a funcation model fro Dogs&cats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout,Input\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 150, 150, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 75, 75, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 75, 75, 64)        9280      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 38, 38, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 38, 38, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 19, 19, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 19, 19, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               6554112   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 6,851,073\n",
      "Trainable params: 6,851,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape=(150,150,3)\n",
    "img_input = Input(shape=input_shape)\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(img_input)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x= Flatten()(x)\n",
    "x = Dense(512,activation='relu')(x)\n",
    "x = Dense(128,activation='relu')(x)\n",
    "output = Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "model = Model(img_input, output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 39s 388ms/step - loss: 0.6843 - accuracy: 0.5513 - val_loss: 0.6888 - val_accuracy: 0.5720\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 38s 384ms/step - loss: 0.6372 - accuracy: 0.6367 - val_loss: 0.5859 - val_accuracy: 0.6880\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 38s 381ms/step - loss: 0.5896 - accuracy: 0.6867 - val_loss: 0.5744 - val_accuracy: 0.7430\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.5459 - accuracy: 0.7183 - val_loss: 0.6984 - val_accuracy: 0.6580\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.5081 - accuracy: 0.7553 - val_loss: 0.4574 - val_accuracy: 0.7300\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 38s 380ms/step - loss: 0.4875 - accuracy: 0.7653 - val_loss: 0.4389 - val_accuracy: 0.7510\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 38s 383ms/step - loss: 0.4558 - accuracy: 0.7893 - val_loss: 0.7348 - val_accuracy: 0.6510\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 38s 384ms/step - loss: 0.4207 - accuracy: 0.8100 - val_loss: 0.5343 - val_accuracy: 0.7650\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 38s 380ms/step - loss: 0.3933 - accuracy: 0.8217 - val_loss: 0.6867 - val_accuracy: 0.7740\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 38s 382ms/step - loss: 0.3601 - accuracy: 0.8410 - val_loss: 0.5182 - val_accuracy: 0.7630\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=100, epochs=10,\n",
    "                              validation_data=val_generator, validation_steps=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Evaluate on the test data\n",
      "760/760 [==============================] - 2s 3ms/step\n",
      "loss:76.6749 accuracy:0.7224\n"
     ]
    }
   ],
   "source": [
    "model.predict(test_imgs)\n",
    "print('\\n#Evaluate on the test data')\n",
    "loss, accuracy = model.evaluate(test_imgs,test_labels_enc)\n",
    "print('loss:%.4f accuracy:%.4f'%(loss,accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex2. butterfly dataset for CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Sequential model on butterfly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from keras.models import *\n",
    "from keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Dropout, Activation, Flatten, Dense, UpSampling2D\n",
    "from keras.models import *\n",
    "from keras.optimizers import SGD,Adam,Adagrad,Nadam,RMSprop\n",
    "from keras.callbacks import TensorBoard,ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "IM_HEIGHT, IM_WIDTH, IM_CHANNEL = 150, 150, 3\n",
    "_data_path = './asset3/data/'\n",
    "train_data_dir = _data_path+\"train/\"\n",
    "valid_data_dir = _data_path+\"valid/\"\n",
    "test_data_dir = _data_path+\"test/\"\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "val_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 244 images belonging to 2 classes.\n",
      "Found 65 images belonging to 2 classes.\n",
      "Found 10 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(IM_HEIGHT, IM_WIDTH),\n",
    "        batch_size=100,\n",
    "        class_mode=\"binary\"\n",
    "    )\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "        valid_data_dir,\n",
    "        target_size=(IM_HEIGHT, IM_WIDTH),\n",
    "        batch_size=100,\n",
    "        class_mode=\"binary\"\n",
    "    )\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(IM_HEIGHT, IM_WIDTH),\n",
    "        batch_size=100,\n",
    "        class_mode=\"binary\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 148, 148, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 74, 74, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 72, 72, 64)        9280      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,706,113\n",
      "Trainable params: 3,706,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (150, 150, 3)\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, kernel_size=(3, 3), activation='relu', \n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "50/50 [==============================] - 130s 3s/step - loss: 0.6368 - accuracy: 0.6297 - val_loss: 0.6306 - val_accuracy: 0.6923\n",
      "Epoch 2/3\n",
      "50/50 [==============================] - 131s 3s/step - loss: 0.5553 - accuracy: 0.7290 - val_loss: 0.5876 - val_accuracy: 0.6615\n",
      "Epoch 3/3\n",
      "50/50 [==============================] - 129s 3s/step - loss: 0.4141 - accuracy: 0.8362 - val_loss: 0.5505 - val_accuracy: 0.7077\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator, steps_per_epoch=50, epochs=3,\n",
    "                              validation_data=val_generator, validation_steps=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 10s 204ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8949041366577148, 0.5]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_generator(test_generator)\n",
    "model.evaluate_generator(test_generator,verbose=1,steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b.functional model on butterfly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout,Input\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 150, 150, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 75, 75, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 75, 75, 64)        9280      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 38, 38, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 38, 38, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 19, 19, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 19, 19, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               6554112   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 6,851,073\n",
      "Trainable params: 6,851,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#bulid functional model\n",
    "input_shape=(150,150,3)\n",
    "img_input = Input(shape=input_shape)\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(img_input)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x= Flatten()(x)\n",
    "x = Dense(512,activation='relu')(x)\n",
    "x = Dense(128,activation='relu')(x)\n",
    "output = Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "model = Model(img_input, output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 244 images belonging to 2 classes.\n",
      "Found 65 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#data processing\n",
    "IM_HEIGHT, IM_WIDTH, IM_CHANNEL = 150, 150, 3\n",
    "_data_path = './asset3/data/'\n",
    "train_data_dir = _data_path+\"train/\"\n",
    "valid_data_dir = _data_path+\"valid/\"\n",
    "test_data_dir = _data_path+\"test/\"\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "val_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(IM_HEIGHT, IM_WIDTH),\n",
    "        batch_size=20,\n",
    "        class_mode=\"binary\"\n",
    "    )\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "        valid_data_dir,\n",
    "        target_size=(IM_HEIGHT, IM_WIDTH),\n",
    "        batch_size=30,\n",
    "        class_mode=\"binary\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50/50 [==============================] - 40s 798ms/step - loss: 0.6698 - accuracy: 0.6239 - val_loss: 0.6019 - val_accuracy: 0.6145\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 39s 789ms/step - loss: 0.6538 - accuracy: 0.6303 - val_loss: 0.6665 - val_accuracy: 0.6158\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 38s 769ms/step - loss: 0.6079 - accuracy: 0.6630 - val_loss: 0.7035 - val_accuracy: 0.6772\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 56s 1s/step - loss: 0.5294 - accuracy: 0.7395 - val_loss: 0.5287 - val_accuracy: 0.6636\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 80s 2s/step - loss: 0.4753 - accuracy: 0.8088 - val_loss: 0.5760 - val_accuracy: 0.7851\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 79s 2s/step - loss: 0.3865 - accuracy: 0.8440 - val_loss: 0.4536 - val_accuracy: 0.7553\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 77s 2s/step - loss: 0.3198 - accuracy: 0.8846 - val_loss: 0.6347 - val_accuracy: 0.7391\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 50s 1s/step - loss: 0.2409 - accuracy: 0.9167 - val_loss: 0.5139 - val_accuracy: 0.6781\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 38s 768ms/step - loss: 0.1978 - accuracy: 0.9263 - val_loss: 0.9667 - val_accuracy: 0.8167\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 38s 768ms/step - loss: 0.1542 - accuracy: 0.9498 - val_loss: 0.4288 - val_accuracy: 0.8173\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=50, epochs=10,\n",
    "                              validation_data=val_generator, validation_steps=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15671152],\n",
       "       [0.89007056],\n",
       "       [0.17834114],\n",
       "       [0.15271674],\n",
       "       [0.56334084],\n",
       "       [0.0087762 ],\n",
       "       [0.4747217 ],\n",
       "       [0.02336858],\n",
       "       [0.23967247],\n",
       "       [0.01879189]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_generator(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN  \n",
    "4D--->2D(dog&cat datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Conv1D, MaxPooling1D,LSTM,Dense, Dropout, Activation,Input, Embedding,Flatten\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "_asset_path = 'asset2/dog_cat/'\n",
    "\n",
    "#named the directory of data set\n",
    "train_dir = _asset_path+'training_data/'\n",
    "val_dir = _asset_path+'validation_data/'\n",
    "test_dir = _asset_path+'test_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e8a3e7432387>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtrain_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMG_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtrain_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_imgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_imgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_imgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\\\'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "IMG_DIM = (32, 32)\n",
    "\n",
    "# To extract the samples from directory of train\n",
    "train_files = glob.glob(train_dir+\"*\") \n",
    "train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]\n",
    "train_imgs = np.array(train_imgs)\n",
    "train_imgs = train_imgs.reshape((len(train_imgs), np.prod(train_imgs.shape[1:])))\n",
    "print(train_imgs.shape)\n",
    "train_labels = [fn.split('\\\\')[-1].split('.')[0].strip() for fn in train_files]\n",
    "le = LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "train_labels_enc = le.transform(train_labels)\n",
    "print(train_labels_enc[2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 3072, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 3068, 64)          41024     \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling1D)         (None, 767, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 767, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 49088)             0         \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 128)               6283392   \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 8,884,545\n",
      "Trainable params: 8,884,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "embedding_size = 128\n",
    "maxlen = 3072\n",
    "inputs_shape = Input(shape = (3072,))\n",
    "em1 = Embedding(max_features, embedding_size, input_length=maxlen)(inputs_shape)\n",
    "conv1 = Conv1D(64, 5,padding='valid', activation='relu', strides=1)(em1)\n",
    "pool1 = MaxPooling1D(pool_size=4,name='pool1')(conv1)\n",
    "Drop1 = Dropout(0.2)(pool1)\n",
    "# la1 = LSTM(70)(pool1)\n",
    "flatten = Flatten()(Drop1)\n",
    "\n",
    "Dense1 = Dense(128,activation='relu',name='Dense1')(flatten)\n",
    "\n",
    "output = Dense(1,activation='sigmoid',name='output')(Dense1)\n",
    "\n",
    "model = Model(inputs=inputs_shape,outputs=output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xzhu2\\Documents\\ProgramData\\anaconda3\\envs\\AML158736\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2700 samples, validate on 300 samples\n",
      "Epoch 1/5\n",
      "2700/2700 [==============================] - 16s 6ms/step - loss: 1.3703 - accuracy: 0.4856 - val_loss: 0.4397 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "2700/2700 [==============================] - 16s 6ms/step - loss: 0.7447 - accuracy: 0.4444 - val_loss: 0.5969 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "2700/2700 [==============================] - 16s 6ms/step - loss: 0.6999 - accuracy: 0.4611 - val_loss: 0.6953 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/5\n",
      "2700/2700 [==============================] - 17s 6ms/step - loss: 0.6929 - accuracy: 0.5556 - val_loss: 0.6967 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/5\n",
      "2700/2700 [==============================] - 17s 6ms/step - loss: 0.6927 - accuracy: 0.5556 - val_loss: 0.6980 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2554e546108>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(train_imgs, train_labels_enc,\n",
    "          batch_size=500,\n",
    "          epochs=5,\n",
    "          validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D CNN(funcation model) on cifar100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3072)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D,Dense, Dropout, Activation,Input, Embedding,Flatten\n",
    "from keras.datasets import imdb\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model,Sequential\n",
    "from keras.datasets import cifar10\n",
    "from keras.optimizers import SGD,Adam,Adagrad,Nadam,RMSprop\n",
    "# cifar10 = keras.datasets.cifar10\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "\n",
    "train_images = train_images.reshape(-1, 32, 32, 3)\n",
    "train_images = train_images.astype('float32')\n",
    "train_images /= 255\n",
    "train_labels = to_categorical(train_labels, 10)\n",
    "\n",
    "train_images = train_images.reshape((len(train_images), np.prod(train_images.shape[1:])))\n",
    "train_images.shape\n",
    "\n",
    "test_images= test_images.reshape(-1, 32, 32, 3)\n",
    "test_images = test_images.astype('float32')\n",
    "test_images /= 255\n",
    "test_labels = to_categorical(test_labels, 10)\n",
    "\n",
    "test_images = test_images.reshape((len(test_images), np.prod(test_images.shape[1:])))\n",
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 3072, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 3068, 64)          41024     \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling1D)         (None, 767, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 767, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 49088)             0         \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 128)               6283392   \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 8,885,706\n",
      "Trainable params: 8,885,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xzhu2\\Documents\\ProgramData\\anaconda3\\envs\\AML158736\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "45000/45000 [==============================] - 276s 6ms/step - loss: 2.2842 - accuracy: 0.1226 - val_loss: 2.2665 - val_accuracy: 0.1420\n",
      "Epoch 2/2\n",
      "45000/45000 [==============================] - 276s 6ms/step - loss: 2.2543 - accuracy: 0.1420 - val_loss: 2.2522 - val_accuracy: 0.1406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2554fb23448>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features = 20000\n",
    "embedding_size = 128\n",
    "maxlen = 3072\n",
    "inputs_shape = Input(shape = (3072,))\n",
    "em1 = Embedding(max_features, embedding_size, input_length=maxlen)(inputs_shape)\n",
    "conv1 = Conv1D(64, 5,padding='valid', activation='relu', strides=1)(em1)\n",
    "pool1 = MaxPooling1D(pool_size=4,name='pool1')(conv1)\n",
    "Drop1 = Dropout(0.2)(pool1)\n",
    "# la1 = LSTM(70)(pool1)\n",
    "flatten = Flatten()(Drop1)\n",
    "\n",
    "Dense1 = Dense(128,activation='relu',name='Dense1')(flatten)\n",
    "\n",
    "output = Dense(10,activation='softmax',name='output')(Dense1)\n",
    "\n",
    "model = Model(inputs=inputs_shape,outputs=output)\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='Adadelta',\n",
    "              optimizer=RMSprop(lr=1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(train_images, train_labels,\n",
    "          batch_size=500,\n",
    "          epochs=2,\n",
    "          validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 13s 1ms/step\n",
      "[2.250304053115845, 0.14900000393390656]\n"
     ]
    }
   ],
   "source": [
    "model.predict(test_images)\n",
    "print(model.evaluate(test_images,test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 3072, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 3072, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 3068, 64)          41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 767, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 49088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                490890    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 3,091,914\n",
      "Trainable params: 3,091,914\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "embedding_size = 128\n",
    "\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "\n",
    "# Training\n",
    "batch_size = 256\n",
    "epochs = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_size, input_length=3072))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xzhu2\\Documents\\ProgramData\\anaconda3\\envs\\AML158736\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "45000/45000 [==============================] - 370s 8ms/step - loss: 0.3223 - accuracy: 0.8999 - val_loss: 0.3200 - val_accuracy: 0.9000\n",
      "Epoch 2/2\n",
      "45000/45000 [==============================] - 542s 12ms/step - loss: 0.3183 - accuracy: 0.9002 - val_loss: 0.3189 - val_accuracy: 0.9003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x25552a195c8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(train_images, train_labels,\n",
    "          batch_size=batch_size,\n",
    "          epochs=2,\n",
    "           validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 13s 1ms/step\n",
      "[0.3189353539466858, 0.9002788066864014]\n"
     ]
    }
   ],
   "source": [
    "model.predict(test_images)\n",
    "# test\n",
    "print(model.evaluate(test_images,test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
