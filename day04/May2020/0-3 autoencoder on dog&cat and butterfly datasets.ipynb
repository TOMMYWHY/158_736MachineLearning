{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Tommy/opt/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/Tommy/opt/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/Tommy/opt/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/Tommy/opt/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/Tommy/opt/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/Tommy/opt/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
    "from datetime import datetime\n",
    "from keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Dropout, Activation, Flatten, Dense, UpSampling2D\n",
    "from keras.models import *\n",
    "from keras.optimizers import SGD,Adam,Adagrad,Nadam,RMSprop\n",
    "from keras.callbacks import TensorBoard,ModelCheckpoint\n",
    "from keras.models import Model\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "_asset_path = '../datasets and figures/asset2/dog_cat/'\n",
    "train_dir = _asset_path+'training_data/'\n",
    "test_dir = _asset_path+'test_data/'\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "IMG_DIM = (32, 32)\n",
    "\n",
    "\n",
    "train_files = glob.glob(train_dir+\"*\") \n",
    "train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]\n",
    "train_imgs = np.array(train_imgs)\n",
    "\n",
    "test_files = glob.glob(test_dir+'*')\n",
    "test_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in test_files]\n",
    "test_imgs = np.array(test_imgs)\n",
    "print(train_imgs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 32)        18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 8, 8, 16)          4624      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8, 8, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 4, 4, 16)          2320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4, 4, 16)          64        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 8, 8, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 8, 8, 32)          4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 32, 32, 3)         1731      \n",
      "=================================================================\n",
      "Total params: 52,707\n",
      "Trainable params: 52,387\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# def simpleCNN(input_shape=(48, 48, 3), classes=1):\n",
    "input_shape=(32, 32, 3)\n",
    "img_input = Input(shape=input_shape)\n",
    "# bn_axis = 3\n",
    "\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(img_input)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "model = Model(img_input, decoded)\n",
    "model.summary()\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Tommy/opt/anaconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 2700 samples, validate on 300 samples\n",
      "Epoch 1/5\n",
      "2700/2700 [==============================] - 17s 6ms/step - loss: -217.6224 - val_loss: -1631.9128\n",
      "Epoch 2/5\n",
      "2700/2700 [==============================] - 16s 6ms/step - loss: -593.9862 - val_loss: -1645.4814\n",
      "Epoch 3/5\n",
      "2700/2700 [==============================] - 16s 6ms/step - loss: -911.9609 - val_loss: -1731.0789\n",
      "Epoch 4/5\n",
      "2700/2700 [==============================] - 15s 6ms/step - loss: -1209.2124 - val_loss: -1695.8119\n",
      "Epoch 5/5\n",
      "2700/2700 [==============================] - 15s 6ms/step - loss: -1476.5948 - val_loss: -1606.3620\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(train_imgs, train_imgs,\n",
    "                epochs=5,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) funcation model on dog&cat dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 67500)\n"
     ]
    }
   ],
   "source": [
    "_asset_path = '../datasets and figures/asset2/dog_cat/'\n",
    "\n",
    "\n",
    "# _asset_path = 'asset2/dog_cat/'\n",
    "train_dir = _asset_path+'training_data/'\n",
    "IMG_DIM = (150, 150)\n",
    "\n",
    "\n",
    "train_files = glob.glob(train_dir+\"*\") \n",
    "train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]\n",
    "train_imgs = np.array(train_imgs)\n",
    "train_imgs = train_imgs.reshape((len(train_imgs), np.prod(train_imgs.shape[1:])))\n",
    "\n",
    "test_files = glob.glob(test_dir+'*')\n",
    "test_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in test_files]\n",
    "test_imgs = np.array(test_imgs)\n",
    "test_imgs = test_imgs.reshape((len(test_imgs), np.prod(test_imgs.shape[1:])))\n",
    "\n",
    "print(train_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 67500)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 784)               52920784  \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                704       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 784)               101136    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 67500)             52987500  \n",
      "=================================================================\n",
      "Total params: 106,127,830\n",
      "Trainable params: 106,127,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_img = Input(shape = (67500,))\n",
    "encoded = Dense(784,activation='relu')(input_img)\n",
    "encoded = Dense(128,activation='relu')(encoded)\n",
    "encoded = Dense(64,activation='relu')(encoded)\n",
    "encoder_output  = Dense(10)(encoded)\n",
    "\n",
    "encoded = Dense(64,activation='relu')(encoder_output)\n",
    "encoded = Dense(128,activation='relu')(encoded)\n",
    "encoded = Dense(784,activation='relu')(encoded)\n",
    "encoded = Dense(67500,activation='tanh')(encoded)\n",
    "\n",
    "autoencoder = Model(inputs = input_img, outputs = encoded)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2700 samples, validate on 300 samples\n",
      "Epoch 1/10\n",
      "2700/2700 [==============================] - 53s 19ms/step - loss: 17358.7635 - val_loss: 17537.7530\n",
      "Epoch 2/10\n",
      "2700/2700 [==============================] - 43s 16ms/step - loss: 17336.7343 - val_loss: 17537.7530\n",
      "Epoch 3/10\n",
      "2700/2700 [==============================] - 38s 14ms/step - loss: 17336.7345 - val_loss: 17537.7530\n",
      "Epoch 4/10\n",
      "2700/2700 [==============================] - 40s 15ms/step - loss: 17336.7344 - val_loss: 17537.7530\n",
      "Epoch 5/10\n",
      "2700/2700 [==============================] - 35s 13ms/step - loss: 17336.7343 - val_loss: 17537.7530\n",
      "Epoch 6/10\n",
      "2700/2700 [==============================] - 36s 13ms/step - loss: 17336.7342 - val_loss: 17537.7530\n",
      "Epoch 7/10\n",
      "2700/2700 [==============================] - 46s 17ms/step - loss: 17336.7339 - val_loss: 17537.7530\n",
      "Epoch 8/10\n",
      "2700/2700 [==============================] - 57s 21ms/step - loss: 17336.7347 - val_loss: 17537.7530\n",
      "Epoch 9/10\n",
      "2700/2700 [==============================] - 43s 16ms/step - loss: 17336.7344 - val_loss: 17537.7530\n",
      "Epoch 10/10\n",
      "2700/2700 [==============================] - 46s 17ms/step - loss: 17336.7335 - val_loss: 17537.7530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a2bb4d470>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.predict(test_imgs)\n",
    "autoencoder.compile(optimizer='adam',loss = 'mse')\n",
    "autoencoder.fit(train_imgs,train_imgs,epochs=10,batch_size = 256,shuffle=True,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)Sequential model on dog&cat datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 3072)\n"
     ]
    }
   ],
   "source": [
    "# _asset_path = 'asset2/dog_cat/'\n",
    "\n",
    "train_dir = _asset_path+'training_data/'\n",
    "IMG_DIM = (32,32)\n",
    "\n",
    "\n",
    "train_files = glob.glob(train_dir+\"*\") \n",
    "train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]\n",
    "train_imgs = np.array(train_imgs)\n",
    "train_imgs = train_imgs.reshape((len(train_imgs), np.prod(train_imgs.shape[1:])))\n",
    "\n",
    "test_files = glob.glob(test_dir+'*')\n",
    "test_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in test_files]\n",
    "test_imgs = np.array(test_imgs)\n",
    "test_imgs = test_imgs.reshape((len(test_imgs), np.prod(test_imgs.shape[1:])))\n",
    "print(train_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_17 (Dense)             (None, 3072)              9440256   \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 784)               2409232   \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 64)                704       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 784)               50960     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 3072)              2411520   \n",
      "=================================================================\n",
      "Total params: 14,363,562\n",
      "Trainable params: 14,363,562\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# input_img = Input(shape = (67500,))\n",
    "model.add(Dense(3072, activation='relu',input_shape=(3072,)))\n",
    "model.add(Dense(784, activation='relu'))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "model.add(Dense(784, activation='relu'))\n",
    "model.add(Dense(3072, activation='relu'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2700 samples, validate on 300 samples\n",
      "Epoch 1/5\n",
      "2700/2700 [==============================] - 5s 2ms/step - loss: 21268.0565 - val_loss: 8475.3497\n",
      "Epoch 2/5\n",
      "2700/2700 [==============================] - 4s 2ms/step - loss: 7396.5446 - val_loss: 6999.5694\n",
      "Epoch 3/5\n",
      "2700/2700 [==============================] - 4s 1ms/step - loss: 6645.0522 - val_loss: 6544.6064\n",
      "Epoch 4/5\n",
      "2700/2700 [==============================] - 4s 1ms/step - loss: 6329.6957 - val_loss: 6406.4765\n",
      "Epoch 5/5\n",
      "2700/2700 [==============================] - 4s 1ms/step - loss: 6225.4264 - val_loss: 6317.5285\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 96.10975 ,  89.71912 ,  84.958115, ..., 105.767105,  92.75763 ,\n",
       "         93.832535],\n",
       "       [125.826485, 117.4976  , 111.24248 , ..., 138.35501 , 121.536224,\n",
       "        122.8392  ],\n",
       "       [151.65239 , 142.57458 , 135.46207 , ..., 165.96257 , 146.55322 ,\n",
       "        148.43866 ],\n",
       "       ...,\n",
       "       [122.57028 , 114.439285, 108.26966 , ..., 134.87233 , 118.27931 ,\n",
       "        119.389694],\n",
       "       [111.77508 , 104.86798 ,  99.59521 , ..., 122.5067  , 107.86435 ,\n",
       "        109.349594],\n",
       "       [174.46878 , 163.5136  , 155.11195 , ..., 191.45294 , 168.34673 ,\n",
       "        170.3226  ]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',loss = 'mse')\n",
    "model.fit(train_imgs,train_imgs,epochs=5,batch_size = 256,shuffle=True,validation_split=0.1)\n",
    "model.predict(test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex1 funcation model on butterfly dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
    "from datetime import datetime\n",
    "from keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Dropout, Activation, Flatten, Dense, UpSampling2D\n",
    "from keras.models import *\n",
    "from keras.optimizers import SGD,Adam,Adagrad,Nadam,RMSprop\n",
    "from keras.callbacks import TensorBoard,ModelCheckpoint\n",
    "from keras.models import Model\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244\n",
      "(244, 3072)\n"
     ]
    }
   ],
   "source": [
    "IMG_DIM = (32,32)\n",
    "# _data_path = './asset3/data/'\n",
    "train_data_dir = _data_path+\"train1/\"\n",
    "# valid_data_dir = _data_path+\"male/\"\n",
    "test_data_dir = _data_path+\"test/\"\n",
    "train_files = glob.glob(train_data_dir+\"*\") \n",
    "print(len(train_files))\n",
    "train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]\n",
    "train_imgs = np.array(train_imgs)\n",
    "train_imgs = train_imgs.reshape((len(train_imgs), np.prod(train_imgs.shape[1:])))\n",
    "\n",
    "test_files = glob.glob(test_data_dir+'*')\n",
    "test_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in test_files]\n",
    "test_imgs = np.array(test_imgs)\n",
    "test_imgs = test_imgs.reshape((len(test_imgs), np.prod(test_imgs.shape[1:])))\n",
    "\n",
    "print(train_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jgan\\.conda\\envs\\py3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jgan\\.conda\\envs\\py3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jgan\\.conda\\envs\\py3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 784)               2409232   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                704       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 784)               101136    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3072)              2411520   \n",
      "=================================================================\n",
      "Total params: 5,040,298\n",
      "Trainable params: 5,040,298\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_img = Input(shape = (3072,))\n",
    "encoded = Dense(784,activation='relu')(input_img)\n",
    "encoded = Dense(128,activation='relu')(encoded)\n",
    "encoded = Dense(64,activation='relu')(encoded)\n",
    "encoder_output  = Dense(10)(encoded)\n",
    "\n",
    "encoded = Dense(64,activation='relu')(encoder_output)\n",
    "encoded = Dense(128,activation='relu')(encoded)\n",
    "encoded = Dense(784,activation='relu')(encoded)\n",
    "encoded = Dense(3072,activation='tanh')(encoded)\n",
    "\n",
    "autoencoder = Model(inputs = input_img, outputs = encoded)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jgan\\.conda\\envs\\py3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jgan\\.conda\\envs\\py3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jgan\\.conda\\envs\\py3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\jgan\\.conda\\envs\\py3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 219 samples, validate on 25 samples\n",
      "Epoch 1/10\n",
      "219/219 [==============================] - 3s 13ms/step - loss: -704.9290 - val_loss: -991.2557\n",
      "Epoch 2/10\n",
      "219/219 [==============================] - 1s 6ms/step - loss: -1067.0822 - val_loss: -1130.1061\n",
      "Epoch 3/10\n",
      "219/219 [==============================] - 1s 5ms/step - loss: -1148.2071 - val_loss: -1173.4436\n",
      "Epoch 4/10\n",
      "219/219 [==============================] - 1s 5ms/step - loss: -1178.0336 - val_loss: -1208.4757\n",
      "Epoch 5/10\n",
      "219/219 [==============================] - 1s 5ms/step - loss: -1225.1790 - val_loss: -1262.9855\n",
      "Epoch 6/10\n",
      "219/219 [==============================] - 1s 6ms/step - loss: -1255.4629 - val_loss: -1265.8196\n",
      "Epoch 7/10\n",
      "219/219 [==============================] - 1s 6ms/step - loss: -1288.2391 - val_loss: -1300.7519\n",
      "Epoch 8/10\n",
      "219/219 [==============================] - 1s 5ms/step - loss: -1309.0384 - val_loss: -1299.1933\n",
      "Epoch 9/10\n",
      "219/219 [==============================] - 1s 5ms/step - loss: -1308.2154 - val_loss: -1323.6777\n",
      "Epoch 10/10\n",
      "219/219 [==============================] - 1s 5ms/step - loss: -1335.0444 - val_loss: -1357.7520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23347a01588>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.compile(optimizer='adam',loss = 'binary_crossentropy')\n",
    "autoencoder.fit(train_imgs,train_imgs,epochs=10,batch_size = 10,shuffle=True,validation_split=0.1)\n",
    "autoencoder.predict(test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex2  Sequential model on butterfly dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 3072)              9440256   \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 784)               2409232   \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                704       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 784)               50960     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 3072)              2411520   \n",
      "=================================================================\n",
      "Total params: 14,363,562\n",
      "Trainable params: 14,363,562\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# input_img = Input(shape = (67500,))\n",
    "model.add(Dense(3072, activation='relu',input_shape=(3072,)))\n",
    "model.add(Dense(784, activation='relu'))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "model.add(Dense(784, activation='relu'))\n",
    "model.add(Dense(3072, activation='relu'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 219 samples, validate on 25 samples\n",
      "Epoch 1/5\n",
      "219/219 [==============================] - 5s 21ms/step - loss: -699.4843 - val_loss: -801.6470\n",
      "Epoch 2/5\n",
      "219/219 [==============================] - 3s 13ms/step - loss: -806.0410 - val_loss: -814.3039\n",
      "Epoch 3/5\n",
      "219/219 [==============================] - 3s 12ms/step - loss: -815.7404 - val_loss: -817.9398\n",
      "Epoch 4/5\n",
      "219/219 [==============================] - 3s 13ms/step - loss: -817.3713 - val_loss: -820.6753\n",
      "Epoch 5/5\n",
      "219/219 [==============================] - 3s 13ms/step - loss: -824.0845 - val_loss: -837.3964\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23362b82308>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',loss = 'binary_crossentropy')\n",
    "model.fit(train_imgs,train_imgs,epochs=5,batch_size = 10,shuffle=True,validation_split=0.1)\n",
    "model.predict(test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
