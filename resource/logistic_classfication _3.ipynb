{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import time\n",
    "#import matplotlib.pyplot as plt\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import datasets\n",
    "#from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]\n",
    "#X = (X - np.mean(X, axis = 0))/X.std()  # normalization\n",
    "\n",
    "y = (iris.target != 0) * 1\n",
    "#theta = np.zeros((X.shape[1], 1)) \n",
    "theta = np.zeros(X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$h_\\theta(x) = g(\\theta^Tx)$\n",
    "\n",
    "$z = \\theta^Tx$\n",
    "\n",
    "$g(z) = \\frac{1}{1 - e^{-z}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "z = np.dot(X, theta)\n",
    "h = sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h = g(x\\theta)$\n",
    "\n",
    "$J(\\theta) = \\frac{1}{m}(-y^Tlog(h) - (1-y)^Tlog(1-h)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(h, y):\n",
    "    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} X^T (g(X\\theta)-y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = np.dot(X.T, (h - y)) / y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we update the weights by substracting to them the derivative times the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "theta -= lr * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions\n",
    "By calling the sigmoid function we get the probability that some input x belongs to class 1. Let’s take all probabilities ≥ 0.5 = class 1 and all probabilities < 0 = class 0. This threshold should be defined depending on the business problem we were working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probs(X, theta):\n",
    "    return sigmoid(np.dot(X, theta))\n",
    "def predict(X, theta, threshold=0.5):\n",
    "    return predict_probs(X, theta) >= threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression1:\n",
    "    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=False, verbose=False):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        \n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    def __loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "       \n",
    "                # weights initialization\n",
    "        #self.theta = np.zeros(X.shape[1])\n",
    "        for i in range(self.num_iter):\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.__sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h - y)) / y.size\n",
    "            self.theta -= self.lr * gradient\n",
    "            \n",
    "            if(self.verbose == True and i % 10000 == 0):\n",
    "                z = np.dot(X, self.theta)\n",
    "                h = self.__sigmoid(z)\n",
    "                print(f'loss: {self.__loss(h, y)} \\t')\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "    \n",
    "        return self.__sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return self.predict_prob(X) >= threshold\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LogisticRegression1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model1.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds == y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "Repeat this code by using the intercept term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import time\n",
    "#import matplotlib.pyplot as plt\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import datasets\n",
    "#from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]\n",
    "\n",
    "y = (iris.target != 0) * 1\n",
    "\n",
    "m = len(y)\n",
    "X0 = np.ones(m)\n",
    "Xpre = X\n",
    "X = np.concatenate((intercept, X), axis=1)\n",
    "#X = np.vstack([X0.T,X.T])\n",
    "#X = X.T\n",
    "\n",
    "#theta = np.zeros((X.shape[1], 1)) \n",
    "theta = np.zeros(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 1)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intercept = np.ones((X.shape[0], 1))\n",
    "intercept.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "Use built function to conduct optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import time\n",
    "from scipy.optimize import fmin_tnc\n",
    "#import matplotlib.pyplot as plt\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import datasets\n",
    "#from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]\n",
    "#X = (X - np.mean(X, axis = 0))/X.std()  # normalization\n",
    "\n",
    "y = (iris.target != 0) * 1\n",
    "#theta = np.zeros((X.shape[1], 1)) \n",
    "theta = np.zeros(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # Activation function used to map any real value between 0 and 1\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def net_input(theta, x):\n",
    "    # Computes the weighted sum of inputs\n",
    "    return np.dot(x, theta)\n",
    "\n",
    "def probability(theta, x):\n",
    "    # Returns the probability after passing through sigmoid\n",
    "    return sigmoid(net_input(theta, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(theta, x, y):\n",
    "    # Computes the cost function for all the training samples\n",
    "    m = x.shape[0]\n",
    "    total_cost = -(1 / m) * np.sum(\n",
    "        y * np.log(probability(theta, x)) + (1 - y) * np.log(\n",
    "            1 - probability(theta, x)))\n",
    "    return total_cost\n",
    "\n",
    "def gradient(theta, x, y):\n",
    "    # Computes the gradient of the cost function at the point theta\n",
    "    m = x.shape[0]\n",
    "    return (1 / m) * np.dot(x.T, sigmoid(net_input(theta,   x)) - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit(x, y, theta):\n",
    "    opt_weights = fmin_tnc(func=cost_function, x0=theta,\n",
    "                  fprime=gradient,args=(x, y.flatten()))\n",
    "    return opt_weights[0]\n",
    "parameters = fit(X, y, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  7.57842852 -13.22112074]\n"
     ]
    }
   ],
   "source": [
    "print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.33333333333333"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(x):\n",
    "    theta = parameters[:, np.newaxis]\n",
    "    return probability(theta, x)\n",
    "def accuracy( x, actual_classes, probab_threshold=0.5):\n",
    "    predicted_classes = (predict(x) >= \n",
    "                         probab_threshold).astype(int)\n",
    "    predicted_classes = predicted_classes.flatten()\n",
    "    accuracy = np.mean(predicted_classes == actual_classes)\n",
    "    return accuracy * 100\n",
    "accuracy(X, y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LogisticRegression1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model1.predict(Xpre,0.5)\n",
    "(preds == y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3:\n",
    "Use scikit-learng to conduct logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1.29953553]), array([[ 3.88395951, -3.41339666]]))"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "preds = model.predict(X)\n",
    "score = model.score(X, y)\n",
    "print(score)\n",
    "model.intercept_, model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score \n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "predicted_classes = model.predict(X)\n",
    "accuracy = accuracy_score(y.flatten(),predicted_classes)\n",
    "parameters = model.coef_\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
