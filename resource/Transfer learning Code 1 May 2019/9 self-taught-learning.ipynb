{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import struct\n",
    "import array\n",
    "import numpy\n",
    "import math\n",
    "import time\n",
    "import scipy.io\n",
    "import scipy.optimize\n",
    "import matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" The Sparse Autoencoder class \"\"\"\n",
    "\n",
    "class SparseAutoencoder(object):\n",
    "\n",
    "    #######################################################################################\n",
    "    \"\"\" Initialization of Autoencoder object \"\"\"\n",
    "\n",
    "    def __init__(self, visible_size, hidden_size, rho, lamda, beta):\n",
    "    \n",
    "        \"\"\" Initialize parameters of the Autoencoder object \"\"\"\n",
    "    \n",
    "        self.visible_size = visible_size    # number of input units\n",
    "        self.hidden_size = hidden_size      # number of hidden units\n",
    "        self.rho = rho                      # desired average activation of hidden units\n",
    "        self.lamda = lamda                  # weight decay parameter\n",
    "        self.beta = beta                    # weight of sparsity penalty term\n",
    "        \n",
    "        \"\"\" Set limits for accessing 'theta' values \"\"\"\n",
    "        \n",
    "        self.limit0 = 0\n",
    "        self.limit1 = hidden_size * visible_size\n",
    "        self.limit2 = 2 * hidden_size * visible_size\n",
    "        self.limit3 = 2 * hidden_size * visible_size + hidden_size\n",
    "        self.limit4 = 2 * hidden_size * visible_size + hidden_size + visible_size\n",
    "        \n",
    "        \"\"\" Initialize Neural Network weights randomly\n",
    "            W1, W2 values are chosen in the range [-r, r] \"\"\"\n",
    "        \n",
    "        r = math.sqrt(6) / math.sqrt(visible_size + hidden_size + 1)\n",
    "        \n",
    "        rand = numpy.random.RandomState(int(time.time()))\n",
    "        \n",
    "        W1 = numpy.asarray(rand.uniform(low = -r, high = r, size = (hidden_size, visible_size)))\n",
    "        W2 = numpy.asarray(rand.uniform(low = -r, high = r, size = (visible_size, hidden_size)))\n",
    "        \n",
    "        \"\"\" Bias values are initialized to zero \"\"\"\n",
    "        \n",
    "        b1 = numpy.zeros((hidden_size, 1))\n",
    "        b2 = numpy.zeros((visible_size, 1))\n",
    "\n",
    "        \"\"\" Create 'theta' by unrolling W1, W2, b1, b2 \"\"\"\n",
    "\n",
    "        self.theta = numpy.concatenate((W1.flatten(), W2.flatten(),\n",
    "                                        b1.flatten(), b2.flatten()))\n",
    "#######################################################################################\n",
    "    \"\"\" Returns elementwise sigmoid output of input array \"\"\"\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "    \n",
    "        return (1 / (1 + numpy.exp(-x)))\n",
    "#######################################################################################\n",
    "    \"\"\" Returns gradient of 'theta' using Backpropagation algorithm \"\"\"\n",
    "        \n",
    "    def sparseAutoencoderCost(self, theta, input):\n",
    "        \n",
    "        \"\"\" Extract weights and biases from 'theta' input \"\"\"\n",
    "        \n",
    "        W1 = theta[self.limit0 : self.limit1].reshape(self.hidden_size, self.visible_size)\n",
    "        W2 = theta[self.limit1 : self.limit2].reshape(self.visible_size, self.hidden_size)\n",
    "        b1 = theta[self.limit2 : self.limit3].reshape(self.hidden_size, 1)\n",
    "        b2 = theta[self.limit3 : self.limit4].reshape(self.visible_size, 1)\n",
    "        \n",
    "        \"\"\" Compute output layers by performing a feedforward pass\n",
    "            Computation is done for all the training inputs simultaneously \"\"\"\n",
    "        \n",
    "        hidden_layer = self.sigmoid(numpy.dot(W1, input) + b1)\n",
    "        output_layer = self.sigmoid(numpy.dot(W2, hidden_layer) + b2)\n",
    "        \n",
    "        \"\"\" Estimate the average activation value of the hidden layers \"\"\"\n",
    "        \n",
    "        rho_cap = numpy.sum(hidden_layer, axis = 1) / input.shape[1]\n",
    "        \n",
    "        \"\"\" Compute intermediate difference values using Backpropagation algorithm \"\"\"\n",
    "        \n",
    "        diff = output_layer - input\n",
    "        \n",
    "        sum_of_squares_error = 0.5 * numpy.sum(numpy.multiply(diff, diff)) / input.shape[1]\n",
    "        weight_decay         = 0.5 * self.lamda * (numpy.sum(numpy.multiply(W1, W1)) +\n",
    "                                                   numpy.sum(numpy.multiply(W2, W2)))\n",
    "        KL_divergence        = self.beta * numpy.sum(self.rho * numpy.log(self.rho / rho_cap) +\n",
    "                                                    (1 - self.rho) * numpy.log((1 - self.rho) / (1 - rho_cap)))\n",
    "        cost                 = sum_of_squares_error + weight_decay + KL_divergence\n",
    "        \n",
    "        KL_div_grad = self.beta * (-(self.rho / rho_cap) + ((1 - self.rho) / (1 - rho_cap)))\n",
    "        \n",
    "        del_out = numpy.multiply(diff, numpy.multiply(output_layer, 1 - output_layer))\n",
    "        del_hid = numpy.multiply(numpy.dot(numpy.transpose(W2), del_out) + numpy.transpose(numpy.matrix(KL_div_grad)), \n",
    "                                 numpy.multiply(hidden_layer, 1 - hidden_layer))\n",
    "        \n",
    "        \"\"\" Compute the gradient values by averaging partial derivatives\n",
    "            Partial derivatives are averaged over all training examples \"\"\"\n",
    "            \n",
    "        W1_grad = numpy.dot(del_hid, numpy.transpose(input))\n",
    "        W2_grad = numpy.dot(del_out, numpy.transpose(hidden_layer))\n",
    "        b1_grad = numpy.sum(del_hid, axis = 1)\n",
    "        b2_grad = numpy.sum(del_out, axis = 1)\n",
    "            \n",
    "        W1_grad = W1_grad / input.shape[1] + self.lamda * W1\n",
    "        W2_grad = W2_grad / input.shape[1] + self.lamda * W2\n",
    "        b1_grad = b1_grad / input.shape[1]\n",
    "        b2_grad = b2_grad / input.shape[1]\n",
    "        \n",
    "        \"\"\" Transform numpy matrices into arrays \"\"\"\n",
    "        \n",
    "        W1_grad = numpy.array(W1_grad)\n",
    "        W2_grad = numpy.array(W2_grad)\n",
    "        b1_grad = numpy.array(b1_grad)\n",
    "        b2_grad = numpy.array(b2_grad)\n",
    "        \n",
    "        \"\"\" Unroll the gradient values and return as 'theta' gradient \"\"\"\n",
    "        \n",
    "        theta_grad = numpy.concatenate((W1_grad.flatten(), W2_grad.flatten(),\n",
    "                                        b1_grad.flatten(), b2_grad.flatten()))\n",
    "                                        \n",
    "        return [cost, theta_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "\"\"\" The Softmax Regression class \"\"\"\n",
    "\n",
    "class SoftmaxRegression(object):\n",
    "\n",
    "    #######################################################################################\n",
    "    \"\"\" Initialization of Regressor object \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, num_classes, lamda):\n",
    "    \n",
    "        \"\"\" Initialize parameters of the Regressor object \"\"\"\n",
    "    \n",
    "        self.input_size  = input_size  # input vector size\n",
    "        self.num_classes = num_classes # number of classes\n",
    "        self.lamda       = lamda       # weight decay parameter\n",
    "        \n",
    "        \"\"\" Randomly initialize the class weights \"\"\"\n",
    "        \n",
    "        rand = numpy.random.RandomState(int(time.time()))\n",
    "        \n",
    "        self.theta = 0.005 * numpy.asarray(rand.normal(size = (num_classes*input_size, 1)))\n",
    "      \n",
    "    #######################################################################################\n",
    "    \"\"\" Returns the groundtruth matrix for a set of labels \"\"\"\n",
    "        \n",
    "    def getGroundTruth(self, labels):\n",
    "    \n",
    "        \"\"\" Prepare data needed to construct groundtruth matrix \"\"\"\n",
    "    \n",
    "        labels = numpy.array(labels).flatten()\n",
    "        data   = numpy.ones(len(labels))\n",
    "        indptr = numpy.arange(len(labels)+1)\n",
    "        \n",
    "        \"\"\" Compute the groundtruth matrix and return \"\"\"\n",
    "        \n",
    "        ground_truth = scipy.sparse.csr_matrix((data, labels, indptr))\n",
    "        ground_truth = numpy.transpose(ground_truth.todense())\n",
    "        \n",
    "        return ground_truth\n",
    "    #######################################################################################\n",
    "    \"\"\" Returns the cost and gradient of 'theta' at a particular 'theta' \"\"\"\n",
    "        \n",
    "    def softmaxCost(self, theta, input, labels):\n",
    "    \n",
    "        \"\"\" Compute the groundtruth matrix \"\"\"\n",
    "    \n",
    "        ground_truth = self.getGroundTruth(labels)\n",
    "        \n",
    "        \"\"\" Reshape 'theta' for ease of computation \"\"\"\n",
    "        \n",
    "        theta = theta.reshape(self.num_classes, self.input_size)\n",
    "        \n",
    "        \"\"\" Compute the class probabilities for each example \"\"\"\n",
    "        \n",
    "        theta_x       = numpy.dot(theta, input)\n",
    "        hypothesis    = numpy.exp(theta_x)      \n",
    "        probabilities = hypothesis / numpy.sum(hypothesis, axis = 0)\n",
    "        \n",
    "        \"\"\" Compute the traditional cost term \"\"\"\n",
    "        \n",
    "        cost_examples    = numpy.multiply(ground_truth, numpy.log(probabilities))\n",
    "        traditional_cost = -(numpy.sum(cost_examples) / input.shape[1])\n",
    "        \n",
    "        \"\"\" Compute the weight decay term \"\"\"\n",
    "        \n",
    "        theta_squared = numpy.multiply(theta, theta)\n",
    "        weight_decay  = 0.5 * self.lamda * numpy.sum(theta_squared)\n",
    "        \n",
    "        \"\"\" Add both terms to get the cost \"\"\"\n",
    "        \n",
    "        cost = traditional_cost + weight_decay\n",
    "        \n",
    "        \"\"\" Compute and unroll 'theta' gradient \"\"\"\n",
    "        \n",
    "        theta_grad = -numpy.dot(ground_truth - probabilities, numpy.transpose(input))\n",
    "        theta_grad = theta_grad / input.shape[1] + self.lamda * theta\n",
    "        theta_grad = numpy.array(theta_grad)\n",
    "        theta_grad = theta_grad.flatten()\n",
    "        \n",
    "        return [cost, theta_grad]\n",
    "    \n",
    "#######################################################################################\n",
    "    \"\"\" Returns predicted classes for a set of inputs \"\"\"\n",
    "            \n",
    "    def softmaxPredict(self, theta, input):\n",
    "    \n",
    "        \"\"\" Reshape 'theta' for ease of computation \"\"\"\n",
    "    \n",
    "        theta = theta.reshape(self.num_classes, self.input_size)\n",
    "        \n",
    "        \"\"\" Compute the class probabilities for each example \"\"\"\n",
    "        \n",
    "        theta_x       = numpy.dot(theta, input)\n",
    "        hypothesis    = numpy.exp(theta_x)      \n",
    "        probabilities = hypothesis / numpy.sum(hypothesis, axis = 0)\n",
    "        \n",
    "        \"\"\" Give the predictions based on probability values \"\"\"\n",
    "        \n",
    "        predictions = numpy.zeros((input.shape[1], 1))\n",
    "        predictions[:, 0] = numpy.argmax(probabilities, axis = 0)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "\"\"\" Loads the images from the provided file name \"\"\"\n",
    "\n",
    "def loadMNISTImages(file_name):\n",
    "\n",
    "    \"\"\" Open the file \"\"\"\n",
    "\n",
    "    image_file = open(file_name, 'rb')\n",
    "    \n",
    "    \"\"\" Read header information from the file \"\"\"\n",
    "    \n",
    "    head1 = image_file.read(4)\n",
    "    head2 = image_file.read(4)\n",
    "    head3 = image_file.read(4)\n",
    "    head4 = image_file.read(4)\n",
    "    \n",
    "    \"\"\" Format the header information for useful data \"\"\"\n",
    "    \n",
    "    num_examples = struct.unpack('>I', head2)[0]\n",
    "    num_rows     = struct.unpack('>I', head3)[0]\n",
    "    num_cols     = struct.unpack('>I', head4)[0]\n",
    "    \n",
    "    \"\"\" Initialize dataset as array of zeros \"\"\"\n",
    "    \n",
    "    dataset = numpy.zeros((num_rows*num_cols, num_examples))\n",
    "    \n",
    "    \"\"\" Read the actual image data \"\"\"\n",
    "    \n",
    "    images_raw  = array.array('B', image_file.read())\n",
    "    image_file.close()\n",
    "    \n",
    "    \"\"\" Arrange the data in columns \"\"\"\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "    \n",
    "        limit1 = num_rows * num_cols * i\n",
    "        limit2 = num_rows * num_cols * (i + 1)\n",
    "        \n",
    "        dataset[:, i] = images_raw[limit1 : limit2]\n",
    "    \n",
    "    \"\"\" Normalize and return the dataset \"\"\"    \n",
    "            \n",
    "    return dataset / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "\"\"\" Loads the image labels from the provided file name \"\"\"\n",
    "    \n",
    "def loadMNISTLabels(file_name):\n",
    "\n",
    "    \"\"\" Open the file \"\"\"\n",
    "\n",
    "    label_file = open(file_name, 'rb')\n",
    "    \n",
    "    \"\"\" Read header information from the file \"\"\"\n",
    "    \n",
    "    head1 = label_file.read(4)\n",
    "    head2 = label_file.read(4)\n",
    "    \n",
    "    \"\"\" Format the header information for useful data \"\"\"\n",
    "    \n",
    "    num_examples = struct.unpack('>I', head2)[0]\n",
    "    \n",
    "    \"\"\" Initialize data labels as array of zeros \"\"\"\n",
    "    \n",
    "    labels = numpy.zeros((num_examples, 1), dtype = numpy.int)\n",
    "    \n",
    "    \"\"\" Read the label data \"\"\"\n",
    "    \n",
    "    labels_raw = array.array('b', label_file.read())\n",
    "    label_file.close()\n",
    "    \n",
    "    \"\"\" Copy and return the label data \"\"\"\n",
    "    \n",
    "    labels[:, 0] = labels_raw[:]\n",
    "    \n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "\"\"\" Visualizes the obtained optimal W1 values as images \"\"\"\n",
    "\n",
    "def visualizeW1(opt_W1, vis_patch_side, hid_patch_side):\n",
    "\n",
    "    \"\"\" Add the weights as a matrix of images \"\"\"\n",
    "    \n",
    "    figure, axes = matplotlib.pyplot.subplots(nrows = hid_patch_side,\n",
    "                                              ncols = hid_patch_side)\n",
    "    index = 0\n",
    "                                              \n",
    "    for axis in axes.flat:\n",
    "    \n",
    "        \"\"\" Add row of weights as an image to the plot \"\"\"\n",
    "    \n",
    "        image = axis.imshow(opt_W1[index, :].reshape(vis_patch_side, vis_patch_side),\n",
    "                            cmap = matplotlib.pyplot.cm.gray, interpolation = 'nearest')\n",
    "        axis.set_frame_on(False)\n",
    "        axis.set_axis_off()\n",
    "        index += 1\n",
    "        \n",
    "    \"\"\" Show the obtained plot \"\"\"  \n",
    "        \n",
    "    matplotlib.pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "\"\"\" Returns the hidden layer activations of the Autoencoder \"\"\"\n",
    "\n",
    "def feedForwardAutoencoder(theta, hidden_size, visible_size, input):\n",
    "\n",
    "    \"\"\" Define limits to access useful data \"\"\"\n",
    "\n",
    "    limit0 = 0\n",
    "    limit1 = hidden_size * visible_size\n",
    "    limit2 = 2 * hidden_size * visible_size\n",
    "    limit3 = 2 * hidden_size * visible_size + hidden_size\n",
    "    \n",
    "    \"\"\" Access W1 and b1 from 'theta' \"\"\"\n",
    "    \n",
    "    W1 = theta[limit0 : limit1].reshape(hidden_size, visible_size)\n",
    "    b1 = theta[limit2 : limit3].reshape(hidden_size, 1)\n",
    "    \n",
    "    \"\"\" Compute the hidden layer activations \"\"\"\n",
    "    \n",
    "    hidden_layer = 1 / (1 + numpy.exp(-(numpy.dot(W1, input) + b1)))\n",
    "    \n",
    "    return hidden_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "\"\"\" Loads data, trains the Autoencoder and Regressor, tests the accuracy \"\"\"\n",
    "\n",
    "def selfTaughtLearning():\n",
    "\n",
    "    \"\"\" Define the parameters of the Autoencoder \"\"\"\n",
    "    \n",
    "    vis_patch_side = 28     # side length of sampled image patches\n",
    "    hid_patch_side = 14     # side length of representative image patches\n",
    "    rho            = 0.1    # desired average activation of hidden units\n",
    "    lamda          = 0.003  # weight decay parameter\n",
    "    beta           = 3      # weight of sparsity penalty term\n",
    "    max_iterations = 400    # number of optimization iterations\n",
    "\n",
    "    visible_size = vis_patch_side * vis_patch_side  # number of input units\n",
    "    hidden_size  = hid_patch_side * hid_patch_side  # number of hidden units\n",
    "    \n",
    "    \"\"\" Load MNIST images for training and testing \"\"\"\n",
    "    \n",
    "    mnist_data    = loadMNISTImages('train-images.idx3-ubyte')\n",
    "    mnist_labels  = loadMNISTLabels('train-labels.idx1-ubyte')\n",
    "    \n",
    "    \"\"\" Assign data of digits 5-9 to Autoencoder \"\"\"\n",
    "    \n",
    "    encoder_set  = numpy.array((mnist_labels >= 5).flatten())\n",
    "    encoder_data = mnist_data[:, encoder_set]\n",
    "    \n",
    "    \"\"\" Initialize the Autoencoder with the above parameters \"\"\"\n",
    "    \n",
    "    encoder = SparseAutoencoder(visible_size, hidden_size, rho, lamda, beta)\n",
    "    \n",
    "    \"\"\" Run the L-BFGS algorithm to get the optimal parameter values \"\"\"\n",
    "    \n",
    "    opt_solution  = scipy.optimize.minimize(encoder.sparseAutoencoderCost, encoder.theta, \n",
    "                                            args = (encoder_data,), method = 'L-BFGS-B', \n",
    "                                            jac = True, options = {'maxiter': max_iterations})\n",
    "    opt_theta     = opt_solution.x\n",
    "    opt_W1        = opt_theta[encoder.limit0 : encoder.limit1].reshape(hidden_size, visible_size)\n",
    "    \n",
    "    \"\"\" Visualize the obtained optimal W1 weights \"\"\"\n",
    "    \n",
    "    visualizeW1(opt_W1, vis_patch_side, hid_patch_side)\n",
    "    \n",
    "    \"\"\" Assign data of digits 0-4 to Softmax \"\"\"\n",
    "    \n",
    "    softmax_set    = numpy.array(((mnist_labels >= 0) & (mnist_labels <= 4)).flatten())\n",
    "    softmax_data   = mnist_data[:, softmax_set]\n",
    "    softmax_labels = mnist_labels[softmax_set, :]\n",
    "    \n",
    "    \"\"\" Split the Softmax set into two halves, one for training and one for testing \"\"\"\n",
    "    \n",
    "    limit        = softmax_data.shape[1]\n",
    "    train_data   = softmax_data[:, :limit]\n",
    "    test_data    = softmax_data[:, limit:]\n",
    "    train_labels = softmax_labels[:limit, :]\n",
    "    test_labels  = softmax_labels[limit:, :]\n",
    "    \n",
    "    \"\"\" Obtain training and testing features from the trained Autoencoder \"\"\"\n",
    "    \n",
    "    train_features = feedForwardAutoencoder(opt_theta, hidden_size, visible_size, train_data)\n",
    "    test_features  = feedForwardAutoencoder(opt_theta, hidden_size, visible_size, test_data)\n",
    "    \n",
    "    \"\"\" Initialize parameters of the Regressor \"\"\"\n",
    "    \n",
    "    input_size     = 196    # input vector size\n",
    "    num_classes    = 5      # number of classes\n",
    "    lamda          = 0.0001 # weight decay parameter\n",
    "    max_iterations = 100    # number of optimization iterations\n",
    "    \n",
    "    regressor = SoftmaxRegression(input_size, num_classes, lamda)\n",
    "    \n",
    "    \"\"\" Run the L-BFGS algorithm to get the optimal parameter values \"\"\"\n",
    "    \n",
    "    opt_solution  = scipy.optimize.minimize(regressor.softmaxCost, regressor.theta, \n",
    "                                            args = (train_features, train_labels,), method = 'L-BFGS-B', \n",
    "                                            jac = True, options = {'maxiter': max_iterations})\n",
    "    opt_theta     = opt_solution.x\n",
    "    \n",
    "    \"\"\" Obtain predictions from the trained model \"\"\"\n",
    "    \n",
    "    predictions = regressor.softmaxPredict(opt_theta, test_features)\n",
    "    \n",
    "    \"\"\" Print accuracy of the trained model \"\"\"\n",
    "    \n",
    "    correct = test_labels[:, 0] == predictions[:, 0]\n",
    "    print (\"\"\"Accuracy :\"\"\", numpy.mean(correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'train-images.idx3-ubyte'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-e9aaf57f4dd5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mselfTaughtLearning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-8afe9221b6c4>\u001b[0m in \u001b[0;36mselfTaughtLearning\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;34m\"\"\" Load MNIST images for training and testing \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mmnist_data\u001b[0m    \u001b[1;33m=\u001b[0m \u001b[0mloadMNISTImages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train-images.idx3-ubyte'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mmnist_labels\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mloadMNISTLabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train-labels.idx1-ubyte'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-695fb207fed2>\u001b[0m in \u001b[0;36mloadMNISTImages\u001b[1;34m(file_name)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;34m\"\"\" Open the file \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mimage_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;34m\"\"\" Read header information from the file \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'train-images.idx3-ubyte'"
     ]
    }
   ],
   "source": [
    "selfTaughtLearning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
